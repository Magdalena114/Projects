{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color='green' size=5> HW #3: What is SoftMax Function?\n",
    "</font> \n",
    "\n",
    "**Customer Analytics** <br>\n",
    "**ANLT 274** <br>\n",
    "**November 10, 2018** <br>\n",
    "<br>\n",
    "\n",
    "<font color='green' size=4> By: Ali Taheri</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SoftMax is an activation function that turns numbers (logits) in the output of neural network into probabilities that sum to one, so we can classify an input to discrete classes unlike the commonly used logistic regression, which can only perform binary classifications. For this target, we would calculate e to the power of every logit and divide all of them by summation of all. This method would preserve the ratio of vectors as opposed to squashing the end-points with a sigmoid as the values saturate. With SoftMax, the biggest value in a vector would have the biggest contribution to output vector too. <br>\n",
    "Let us consider a neural network with following label data: <br>\n",
    "[1,0,0] #man   <br>\n",
    "[0,1,0] #woman   <br>\n",
    "[0,0,1] #bisexual  <br>\n",
    "<br>\n",
    "Input to activation layer is as below: <br>\n",
    "[5 , 3 , 1.5] <br>\n",
    "Now, we calculate probabilities by SoftMax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.round(np.exp(x) / np.sum(np.exp(x)),2)\n",
    "\n",
    "def softmax_1(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x with subtraction of maximum value.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return np.round(e_x / e_x.sum(),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply both function to scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.86 0.12 0.03]\n",
      "[0.86 0.12 0.03]\n"
     ]
    }
   ],
   "source": [
    "scores = [5, 3.0, 1.5]\n",
    "print(softmax(scores))\n",
    "print(softmax_1(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see both outputs are the same as we expected because we divided both of numerator and denominator to the maximum value of logits. We do this for more stability in case of huge numbers and the chance of overflow. <br>\n",
    "\n",
    "The output probabilities are saying 86% sure it is a man, 12% a woman and 3% a bisexual. <br>\n",
    "Now, let us have a simple normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.53, 0.32, 0.16])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(scores / np.sum(scores) , 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
