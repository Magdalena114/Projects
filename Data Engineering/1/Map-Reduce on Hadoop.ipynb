{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color='green' size=5> Lab 1: Map-Reduce on Hadoop  </font> \n",
    "\n",
    "**Data Engineering** <br>\n",
    "**ANLT 214** <br>\n",
    "**September 23, 2018** <br>\n",
    "<br>\n",
    "\n",
    "<font color='green' size=4> By: Ali Taheri</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this lab, we would have an experience with paradigm of Map-Reduce on Hadoop that is running in aws instance of University of the Pacific. \n",
    "We have selecred about 50 S3 log files of Professor.Mike Williamson and we will try to extract some information or insights from them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First phase of our work is as following:\n",
    "\n",
    "<ul>\n",
    "<li>Capture the <a href=\"https://en.wikipedia.org/wiki/List_of_HTTP_status_codes\">HTTP response codes</a> that are in the logs, and perform the following summary:\n",
    "\n",
    "<ol>\n",
    "<li>Get a <strong>list</strong> of all of the <strong>response codes</strong> and <strong>how many times</strong> they occurred.</li>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part, we develop following mapper and reducer. <br>\n",
    "The mapper is as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"mapper.py\n",
    "\n",
    "This code is for extraction of Http codes.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # split the line into words\n",
    "    strings = line.split()\n",
    "    # increase counters\n",
    "    for k in range(len(strings)):\n",
    "        if strings[k][-9:] == 'HTTP/1.1\"' :\n",
    "            # write the results to STDOUT (standard output);\n",
    "            # what we output here will be the input for the\n",
    "            # Reduce step, i.e. the input for reducer.py\n",
    "            #\n",
    "            # tab-delimited; the trivial word count is 1\n",
    "            print(strings[k+1], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All codes are after 'HTTP/1.1', so we try to find all instances of it and then we could extract respective codes. <br>\n",
    "The reducer is as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"reducer.py\n",
    "\n",
    "This code is for enumeration of codes.\n",
    "\"\"\"\n",
    "\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "current_code = None\n",
    "current_count = 0\n",
    "count = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    #word, count = line.split('\\t', 1)\n",
    "    code, count = line.split()\n",
    "\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_code == code:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_code:\n",
    "            # write result to STDOUT\n",
    "            print (current_code, current_count)\n",
    "        current_count = count\n",
    "        current_code = code\n",
    "\n",
    "# do not forget to output the last word if needed!\n",
    "if current_code == code:\n",
    "    print (current_code, current_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we go for runing map reduce for all of our log files. Command is as below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yarn jar /home/ec2-user/hadoop-3.1.1/share/hadoop/tools/lib/hadoop-streaming-3.1.1.jar\n",
    "-files /home/ec2-user/hadoop-3.1.1/ali/http_classification/mapper.py,\n",
    "       /home/ec2-user/hadoop-3.1.1/ali/http_classification/reducer.py \n",
    "-mapper /home/ec2-user/hadoop-3.1.1/ali/http_classification/mapper.py \n",
    "-reducer /home/ec2-user/hadoop-3.1.1/ali/http_classification/reducer.py \n",
    "-input ali_logfiles/* \n",
    "-output ali_http_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With running above yarn command, result would be written in a new folder with name of \"ali_http_results\" on HDFS and it is as below with running following command:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ec2-user@ip-172-31-10-136 ~]$ hdfs dfs -cat ali_http_results/part-00000\n",
    "\n",
    "200 792\t\n",
    "204 1\t\n",
    "206 78\t\n",
    "301 7\t\n",
    "307 43\t\n",
    "403 1\t\n",
    "404 3\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report of hadoop for this job is as following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"1.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second part of work is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Then <strong>aggregate</strong> these response codes into their <strong>generalized categories</strong>:\n",
    "\n",
    "<ul>\n",
    "<li><code>1xx</code>: info, <code>2xx</code>: success, <code>3xx</code>: redirection, <code>4xx</code>: client errors, and <code>5xx</code>: server errors</li>\n",
    "<li>So you should have 2 summaries, one that has the individual code &amp; the number of times it occurred, and another that has the generalized higher level code &amp; the number of times it occurred.</li>\n",
    "</ul></li>\n",
    "</ol></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For finding categories, we could do it in above reducer, but we made another reducer as below for more experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"aggregator.py\n",
    "\n",
    "This code is for enumeration of general categories.\n",
    "\"\"\"\n",
    "\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "current_code = None\n",
    "current_count = 0\n",
    "count = None\n",
    "cat_numbers={\"1xx\":0 , \"2xx\":0 , \"3xx\":0 , \"4xx\":0 , \"5xx\":0}\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "   \n",
    "    code, count = line.split()\n",
    "\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    \n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_count:\n",
    "        if current_code[0] == code[0] :\n",
    "            current_count += count\n",
    "            current_code = code\n",
    "        else:\n",
    "            # write result to STDOUT\n",
    "            cat_numbers[current_code[0]+\"xx\"] = current_count\n",
    "            current_count = count\n",
    "            current_code = code\n",
    "           \n",
    "        \n",
    "    else:\n",
    "        current_count = count\n",
    "        current_code = code\n",
    "        \n",
    "\n",
    "# do not forget to output the last code if needed!\n",
    "if current_code[0] == code[0]:\n",
    "    cat_numbers[current_code[0]+\"xx\"] += current_count\n",
    "    \n",
    "total = sum(cat_numbers.values())\n",
    "for cat in cat_numbers:\n",
    "    print(cat ,\":\", cat_numbers[cat] , \"   \"+str(round(100 * cat_numbers[cat] / total))+\"%\")\n",
    "print(\"Total = \" +str(total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we run following yarn commad:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yarn jar /home/ec2-user/hadoop-3.1.1/share/hadoop/tools/lib/hadoop-streaming-3.1.1.jar\n",
    "-files /home/ec2-user/hadoop-3.1.1/ali/http_categorization/mapper.py,\n",
    "/home/ec2-user/hadoop-3.1.1/ali/http_categorization/reducer.py\n",
    "-mapper /home/ec2-user/hadoop-3.1.1/ali/http_categorization/mapper.py\n",
    "-reducer /home/ec2-user/hadoop-3.1.1/ali/http_categorization/reducer.py \n",
    "-input ali_logfiles/* \n",
    "-output ali_http_cat_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running of yarn, results are as following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ec2-user@ip-172-31-10-136 ~]$ hdfs dfs -cat ali_http_cat_results/part-00000\n",
    "\n",
    " 1xx : 000  ------  0%\t\n",
    " 2xx : 871  ------ 94%\t\n",
    " 3xx : 050  ------ 5%\t\n",
    " 4xx : 004  ------ 0%\t\n",
    " 5xx : 000  ------ 0%\t\n",
    " Total = 925"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could see that a lot of http codes are reporing successful operations and we have a tiny number of client errors. <br>\n",
    "Report of hadoop is as below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"2.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this section we want to do a fun part! We want to extract all IP addresses that Professor Mike has connected with those addresses to aws. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li><strong>Discover</strong> my (Prof. Mike&#39;s) <strong>home IP address</strong> and what my <a href=\"https://docs.aws.amazon.com/general/latest/gr/acct-identifiers.html\"><strong>canonical user id</strong></a> is at AWS.\n",
    "\n",
    "<ul>\n",
    "<li>This is perhaps a bit trickier, but maybe more fun, too. Remember what courses I teach (Data Wrangling, Intro to Data Viz, Software Methods, Visual Storytelling, and Dynamic Visualization, along with this course).</li>\n",
    "</ul></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our mapper would be as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"mapper.py\n",
    "\n",
    "This code is for extraction of ip addresses and aws canoical user ID's.\n",
    "Ali Taheri\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # split the line into words\n",
    "    strings = line.split()\n",
    "    # increase counters\n",
    "    for k in range(len(strings)):\n",
    "        if strings[k][-6:] == '+0000]' :\n",
    "            # write the results to STDOUT (standard output);\n",
    "            # what we output here will be the input for the\n",
    "            # Reduce step, i.e. the input for reducer.py\n",
    "            #\n",
    "            # tab-delimited; the trivial word count is 1\n",
    "            print(strings[k+1], 1)\n",
    "        elif ((strings[k][0:4] == \"data\") or (strings[k][0:4] == \"intr\") or (strings[k][0:4] == \"soft\") \\\n",
    "            or (strings[k][0:4] == \"visu\") or (strings[k][0:4] == \"dyna\")) and strings[k][-7:] != \".tar.gz\" :\n",
    "            print(strings[k-1], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above code, we extract all ip addresses and canonical user id's in logfiles. <br>\n",
    "Reducer is as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"reducer.py\n",
    "\n",
    "This code is for enumeration of ip addresses and aws canoical user ID's.\n",
    "Ali Taheri\n",
    "\"\"\"\n",
    "\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "current_code = None\n",
    "current_count = 0\n",
    "count = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    #word, count = line.split('\\t', 1)\n",
    "    code, count = line.split()\n",
    "\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_code == code:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_code:\n",
    "            # write result to STDOUT\n",
    "            print (current_code, current_count)\n",
    "        current_count = count\n",
    "        current_code = code\n",
    "\n",
    "# do not forget to output the last word if needed!\n",
    "if current_code == code:\n",
    "    print (current_code, current_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we could run following yarn command:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yarn jar /home/ec2-user/hadoop-3.1.1/share/hadoop/tools/lib/hadoop-streaming-3.1.1.jar\n",
    "-files /home/ec2-user/hadoop-3.1.1/ali/ip_address/mapper.py,\n",
    "/home/ec2-user/hadoop-3.1.1/ali/ip_address/reducer.py \n",
    "-mapper /home/ec2-user/hadoop-3.1.1/ali/ip_address/mapper.py \n",
    "-reducer /home/ec2-user/hadoop-3.1.1/ali/ip_address/reducer.py \n",
    "-input  ali_logfiles/* \n",
    "-output ali_ip_cid_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are as following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ec2-user@ip-172-31-10-136 ~]$ hdfs dfs -cat ali_ip_cid_results/part-00000\n",
    "\n",
    "10.233.7.37   ----    1\t\n",
    "136.60.153.92  ----    1\t\n",
    "138.9.5.26 ---- 1\t\n",
    "138.9.5.27 ---- 81\t\n",
    "138.9.5.39 ---- 233\t\n",
    "138.9.5.61 ---- 773\t\n",
    "138.9.57.32 ---- 1\t\n",
    "5.189.142.136 ---- 1\t\n",
    "52.95.24.235 ---- 1\t\n",
    "54.212.180.219 ---- 1\t\n",
    "66.192.183.244 ---- 1\t\n",
    "71.199.44.171 ---- 1\t\n",
    "99.73.92.216 ---- 110\t\n",
    "f0a3b2b89cd97cc38e2763d36fd696e8d106c2ed968e0c81643c6688f994076c ---- 1203\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a survey, we could find that subnet of 138.9.x.x is for University of the Pacific in Stockton and it shows that Professor Mike is connecting to aws a lot from Stockton campus of UOP. <br> <br>\n",
    "It seems that ip address of 99.73.92.216 is for house of Professor Mike. It proves that he lives in San Mateo and his internet provider is AT&T (U-verse). <br> <br>\n",
    "\n",
    "Other ip addresses are very rare and maybe he connected to aws from those places. They are from following cities:<br>\n",
    "1- Herriman, Utah <br>\n",
    "2- Salt Lake City, Utah <br>\n",
    "3- Seattle, Washigton <br>\n",
    "4- Boardman, Oregon <br>\n",
    "5- Columbus, Ohio <br>\n",
    "\n",
    "Those ip addresses do not prove that he was in those places, because sometimes providers are using ip addresses in different places. Specially this could happrn in mobile networks. <br> <br>\n",
    "\n",
    "Finally, his canonical user id is \"f0a3b2b89cd97cc38e2763d36fd696e8d106c2ed968e0c81643c6688f994076c\" that has been repeated for 1203 times. <br> <br>\n",
    "\n",
    "Report of Hadoop is as below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"3.jpg\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
