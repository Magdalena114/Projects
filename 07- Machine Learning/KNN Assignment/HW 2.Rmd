---
title: "ANLT-203_01 Assignment 2: K Nearest Neighbor and VDM "
author:  "Brendon Ngo- Joseph Jung - Ali Taheri"
date: "Mar 5, 2018"
output:
  html_document:
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(stats)
library(dplyr)
library(flexclust)
library(caret)
library(knitr)
library(kableExtra)
```


## Problem  1

### 1) Prediction of adherent classes by continous features:

```{r Data reading for KNN, warning=FALSE}

#Reading data to a dataframe
train.df=read.csv("healthcareTrain.csv",header=TRUE)
test.df=read.csv("healthcareTest.csv",header=TRUE)

adTrain= train.df
adTest=test.df
#Selection of interested features
train.df=train.df[,c( "pre_rx_cost", "numofgen","numofbrand", "generic_cost", "adjust_total_30d","num_er","regionN","pdc_80_flag")]
test.df=test.df[,c( "pre_rx_cost", "numofgen","numofbrand", "generic_cost", "adjust_total_30d","num_er","regionN","pdc_80_flag")]

```


```{r Data normalization for KNN, warning=FALSE}

#Normalizing train and test data based on min and max of train data
train.min=apply(train.df[,1:6],2,min)
train.max=apply(train.df[,1:6],2,max)

train.df[,1:6]=train.df[,1:6] %>% sweep(MARGIN=2, STAT=train.min, FUN="-") %>% 
                           sweep(MARGIN=2, STAT=train.max-train.min , FUN= "/")

test.df[,1:6]=test.df[,1:6] %>% sweep(MARGIN=2, STAT=train.min, FUN="-") %>% 
                          sweep(MARGIN=2, STAT=train.max-train.min , FUN= "/")
```


```{r Functions for KNN, warning=FALSE}

#function for calculation of euclidean distances between training and test points
distance <- function(train.points,test.points, ncol){
   dist= dist2(train.points[,1:ncol], test.points[,1:ncol], method = "euclid")
   
  if (ncol < 7) {
    d = dist
  } else {
    vdm=sapply(1:nrow(test.points), function(i) sapply(1:nrow(train.points) , function(j)     VDM.distances[train.points[j,7],test.points[i,7]]))
    d=sqrt(dist^2+vdm)
  }
  return (d) 
}
   
#function for prediction label of each test point based on training data
knn_predict <- function(train.df, test.df, ncol, k_value){
  
prediction=c() #Prediction vector of test points
  
dist=distance( as.matrix(train.df[,1:7]) , as.matrix(test.df[,1:7]) , ncol)
indexedDist = apply(dist,MARGIN=2, order)
sortedDist = apply(dist , MARGIN=2 , sort)
   
   
#Prediction based on normal KNN
prediction = sapply(1:nrow(test.df), function(testNum) ifelse(sum(train.df$pdc_80_flag[indexedDist[1:k_value,testNum]]) >= k_value/2, 1, 0 ))
   
return(prediction) #return prediction vector
}

```


```{r Applying 3-NN with different norms, warning=FALSE}

accuracy.df=data.frame(matrix(c(seq(75,105,2),rep(0,16)),nrow=2,byrow= TRUE))
rownames(accuracy.df) = c("K" , "Accuracy Rate")
colnames(accuracy.df) = sapply(1:16, function(i) paste('K',i))
accuracy.df[2,]=sapply(1:ncol(accuracy.df) , function(i) 100*sum(knn_predict(train.df, test.df, 6, accuracy.df[1,i]) == test.df[,8])/nrow(test.df))

accuracy.df %>%
  kable(digits=3 , format="html") %>%
  kable_styling() %>%
  column_spec(1 , bold=T , color="blue") %>%
  row_spec(0 , bold=T , color="red")

```

### 2) Plotting of accuracy rates vs. K without symbolic feature:

```{r Ploting and reporting best K , warning=FALSE}
accuracy.matrix=as.matrix(accuracy.df)
cat("Mean of accuracy rate withour categorical feature is ",round(mean(accuracy.matrix[2,]),3))
cat("Accuracy rate is maximum for K of",accuracy.matrix[1, which.max(accuracy.matrix[2,])])

plot(accuracy.matrix[1,],accuracy.matrix[2,],col="blue",pch= 18, ylab="Accuracy Rate", xlab="Number of Neighbors", main="Accuracy rates based on K = 75-105")

```

## Problem  2

### 1) Find all the relevant conditional probabilities:

```{r Calculation of conditional probabilities for VDM, warning=FALSE}

cond.prob.per.region =t(sapply(1:4 , function(i) table(train.df[train.df$regionN == i,c(7,8)]) / sum(table(train.df[train.df$regionN == i,c(7,8)]))))

classes = c('Northeast', 'Midwest', 'South',  'West')

rownames(cond.prob.per.region) = classes
colnames(cond.prob.per.region) =c("class 0" , "class 1")

cond.prob.per.region %>%
  kable(digits=3 , format="html") %>%
  kable_styling() %>%
  column_spec(1 , bold=T , color="blue") %>%
  row_spec(0 , bold=T , color="red")
```

### 2) Find the distance between symbolic feature:

```{r Calculation of distances for VDM, warning=FALSE}

VDM.distances= data.frame(matrix(rep(0,16),nrow=4))
rownames(VDM.distances) = classes
colnames(VDM.distances) = classes

VDM.distances=sapply(classes, function(region1) sapply(classes , function(region2 ) 2*(cond.prob.per.region[region1,'class 0']-cond.prob.per.region[region2,'class 0'] )^2) )

delta=VDM.distances
 
VDM.distances %>%
  kable(digits=3 , format="html") %>%
  kable_styling() %>%
  column_spec(1 , bold=T , color="blue") %>%
  row_spec(0 , bold=T , color="red")
```


### 3) Prediction of adherent classes with continous and symbolic feature: 

```{r Classification by continous and categorical, warning=FALSE}
accuracy.df=data.frame(matrix(c(seq(75,105,2),rep(0,16)),nrow=2,byrow= TRUE))
rownames(accuracy.df) = c("K" , "Accuracy Rate")
colnames(accuracy.df) = sapply(1:16, function(i) paste('K',i))
accuracy.df[2,]=sapply(1:ncol(accuracy.df) , function(i) sum(knn_predict(train.df, test.df, 7, accuracy.df[1,i]) == test.df[,8])/nrow(test.df) * 100)

accuracy.df %>%
  kable(digits=2 , format="html") %>%
  kable_styling() %>%
  column_spec(1 , bold=T , color="blue") %>%
  row_spec(0 , bold=T , color="red")

accuracy.matrix=as.matrix(accuracy.df)
cat("Mean of accuracy rate with symbolic feature is ",round(mean(accuracy.matrix[2,]),2))


```

It shows that mean of accuracy rate has decresed by this categorical variable from 66.61 to 66.02 .

### 4) Plotting of accuracy rates vs. K with symbolic feature:

```{r Plot accuracy rate vs. K , warning=FALSE}
cat("Accuracy rate is maximum for K of",accuracy.matrix[1, which.max(accuracy.matrix[2,])])

plot(accuracy.matrix[1,],accuracy.matrix[2,],col="blue",pch= 18,ylab="Accuracy Rate", xlab="Number of Neighbors", main="Accuracy rates based on K = 75-105")
```

### 5) Predicted class for some test points:

```{r Class for 100 200 and 300 th points, warning=FALSE}

prediction = knn_predict(train.df, test.df, 7, which.max(accuracy.matrix[2,]))
index=c(100,200,300)
results =matrix(rep(0,6),nrow=2)
results[1,1:3]=test.df[index,8]
results[2,1:3]=prediction[index]

colnames(results) = c("100th","200th","300th")
rownames(results) = c("Observed" ,"Predicted")

results %>%
  kable(digits=2 , format="html") %>%
  kable_styling() %>%
  column_spec(1 , bold=T , color="blue") %>%
  row_spec(0 , bold=T , color="red")
```

```{r Anahita, warning=FALSE}
#output is in col 94. Take it out.
preObj <- preProcess(adTrain[, -94], method="range")
adTrain.norm <- predict(preObj, adTrain[, -94])
adTest.norm = predict(preObj, adTest[,-94])

contTitle = c("adjust_total_30d",
"generic_cost",
"pre_rx_cost",
"numofgen",
"num_er",
"numofbrand"
)
contTrain = subset(adTrain.norm, select = contTitle)
contTest = subset(adTest.norm, select = contTitle)


#myDist is train size x test size => 1378 x 344
#col 1 (1378x1) is the distance between the first testPoint and all 1378 points in training
#col 2 (1378x1) is the distance between the second testPoint and all 1378 points in training, so on and so forth

myDist = dist2((contTrain), (contTest), method = "euc")

#must square the distance L=sqrt(delta + dist^2)
myDistSq = myDist^2
totalDis = sqrt(myDistSq + delta[adTrain$regionN,adTest$regionN])

#sorting the distance indeces for each column from smallest to largest
inx = apply(totalDis, 2, order)

KNN = seq(75,105, by=2)

predTestLabel=sapply(KNN, function(K)(
sapply(1:nrow(adTest), function(testNum)
ifelse (sum(adTrain$pdc_80_flag[inx[1:K, testNum]]) > K/2, +1, 0)
)
)
)

acc = sapply(1:length(KNN), function(K) mean(predTestLabel[,K]== adTest$pdc_80_flag))


```