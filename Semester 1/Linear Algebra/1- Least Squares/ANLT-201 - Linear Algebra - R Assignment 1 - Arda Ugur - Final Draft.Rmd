---
title: "Predicting NFL Quarterback Ratings^[ANLT-201 - Linear Algebra for Data Science / Assignment 01]"
subtitle: "A Least Squares Method and Linear Regression Implementation in R"
author: "</br>by Arda Ugur, SeyedAli TaheriTari <br/> a_ugur1@u.pacific.edu, s_taheritari@u.pacific.edu </br> </br> University of the Pacific, School of Engineering and Computer Science, Ms. Sci. Data Science</br>"
date: "September 2017"
output: 
 html_document:
  toc: TRUE
  toc_float:
    collapsed: FALSE
  number_sections: TRUE
  theme: lumen
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

 ![](/Users/satah/Downloads/qbs.jpg) 


</br><br/>

# Introduction

[Quarterback rating](https://en.wikipedia.org/wiki/Passer_rating) (also known as __passer rating__, or __QB rating__) is a measure of the performance of passers, primarily quarterbacks, in American football. Quarterback rating is calculated using player's passing attempts, completions, yards, touchdowns, and interceptions. Since 1973, passer rating has been the official formula used by the NFL to determine its passing leader. Passer rating in the NFL is on a scale from 0 to 158.3. 

The main objective of this study is to provide an example of the method of __least squares__ in [R](https://en.wikipedia.org/wiki/Passer_rating) programming language, using the 2008 NFL quarterback ratings data set, provided below.  

```{r raw_data, include=FALSE}
#
# Step 0.1: Read quarterback rating data from .csv file
#
library(readr)
qbratingdata <- read_csv("rating.csv")
```

```{r raw_table, echo=FALSE}
#
# Step 0.2: Print quarterback rating data as reference in the table format
#
knitr::kable(qbratingdata[], caption = "2008 NFL Quarterback Ratings")
```

# Objectives

* __Objective 1__ : Create a least squares model to relate the dependent variable __quarterback rating__ to the __percentage of completions__.
</br><br/>
* __Objective 2__ : Create a least squares model to relate the dependent variable __quarterback rating__ to the __percentage of completions__ and __interceptions__. 
</br><br/>
* __Objective 3__ : Determine the __least squares error__ (or the _square root of sum of square errors_) for models in __Objective 1__ and __Objective 2__. Does using an additional variable (i.e., percentage of interceptions) improve the accuracy of the linear regression model for this data set?
</br><br/>
* __Objective 4__ : Using your models from __Objective 1__ and __Objective 2__ predict the __quarterback rating__ for 60% completion and 3% interception. 
</br><br/>
* __Objective 5__ : Go Seahawks!

# Theory

Regression analysis is a conceptually simple method for investigating functional relationships between two or amoung more than two variables. For example, a civil engineer may wish to relate a specific mixture of concrete specimen's compressive strength to its other physical or mechanical attributes (e.g., compressive strenght versus density, air content, water/cement ratio, etc.) Or an economist may wish to investigate the relationship between the economic structure and environmental pollution level of a country or a region. 
</br><br/>
The investigated relationship is usually expressed in the form of an equation or a model connecting the __response__ (or __dependent__) __variable__ and one or more __explanatory__ or __predictor variables__.We denote the response variable $Y$ and the set pf predictor variables by $X_1$, $X_2$, ..., $X_p$, where $p$ denotes the number of predictor variables. The __true__ relationship between $Y$ and $X_1$, $X_2$, ..., $X_p$ can be approximated by the following regression model:
$$Y=f(X_1, X_2, ..., X_p) + \epsilon$$
where $\epsilon$ is assumed to be a random error representing the discrepancy in the approximation. It accounts for the failure of the model to fit the data exactly. The function $f(X_1, X_2, ..., X_p)$ describes the relationship between $Y$ and $X_1$, $X_2$, ..., $X_p$. An example to such relationship is the __linear regression model__ which can be expressed as: 
$$ Y= \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_pX_p + \epsilon$$ 
where $\beta_0, \beta_1, \beta_2, ...,\beta_p$, called the __regressuion parameters__ or __regression coefficients__, are the unknown constants to be estimated from the data. 
</br><br/>
We can also represent __multiple regression analysis__ (and  __simple regression analysis__) in matrix notation. To do so, first, let us define the following matrices:

\usepackage{amsmath}

\begin{equation}

Y = \begin{bmatrix} 
    y_1 \\
    y_2\\
    \vdots\\
    y_n 
    \end{bmatrix}
\space\space\space\space\
X = \begin{bmatrix} 
    1 & x_{11} & x_{12} & \dots &  x_{1p}\\
    1 & x_{21} & x_{22} & \dots &  x_{2p}\\
    \vdots & \vdots & \vdots &  &  \vdots\\
    1 & x_{n1} & x_{n2} & \dots &  x_{np}\\
    \end{bmatrix}
\space\space\space\space\
\beta = \begin{bmatrix} 
    \beta_1 \\
    \beta_2\\
    \vdots\\
    \beta_p 
    \end{bmatrix}
\space\space\space\space\
\epsilon = \begin{bmatrix} 
    \epsilon_1 \\
    \epsilon_2\\
    \vdots\\
    \epsilon_p 
    \end{bmatrix}
    \end{equation}


The linear model given earlier, then can be expressed in terms of the matrices presented above as
$$ Y = X\beta + \epsilon $$ 
The least squares estimator $\widehat{\beta}$ of $\beta$ is obtained by minimizing the SSE (Sum of Squared Errors or the sum of squared deviations of the observations from their expected values) as below:
$$ SSE=\epsilon_1^2+\epsilon_2^2+...+\epsilon_p^2 $$

Consequently,in the matrix notation, the least squares estimators are obtained by minimizing $S(\beta)$, where 
$$S(\beta) = \epsilon^T\epsilon = (Y - X\beta)^T(Y - X\beta)$$
Since we can only estimate $\beta$, minimization of $S(\beta)$ takes us to the system of equations 
$$(X^TX)\widehat{\beta} = X^TY$$
And finally, provided that $X^TX$ is invertible, the __least squares estimate__ $\widehat{\beta}$ can be written as: 
$$\widehat{\beta}=(X^TX)^{-1}X^TY$$
Our R implementation for this particular assignment will be based on the mathematical relationships given above. For more detailed information mathematical foundations of linear regression analysis, please refer to the [references](#References) at the end of this paper. 


# Implementation

In the following subsections we are going to implement the objectives given in previous sections.

```{r libraries, include=FALSE}
library(MASS)
```

```{r qbrating, include=FALSE}
#
# Step 0.3: Get raw data into matrix format
#
qbrating_matrix <- cbind(qbratingdata)   # we will use this later in further studies
```

## Objective 1

Let us create a least squares model to relate the response variable __quarterback rating__ to the __percentage of completions__

```{r x.a_matrix, echo=FALSE}
X.a <- cbind(rep(1,32), qbratingdata$`Pct Comp`)
print("Matrix for Percentage of Completions:")
print(X.a)
```

```{r y.a_vector, echo=FALSE}
Y.a <- cbind(qbratingdata$`Rating Pts`)
print("Vector for Quarterback Rating Points")
print(Y.a)
```

Using `ginv` function, we are going to compute the vector for $\beta$ that consists regression coefficients of $\beta_0$ and $\beta_1$.

```{r beta.a_vector, echo=FALSE}
library(MASS)
Beta.a <- ginv(X.a)%*%Y.a
print("Regression Coefficients for Quarterback Rating Points based on Percentage of Completions:")
print(Beta.a)
```
Using computed values in previous steps, let us compute the predicted values of Quarterback Rating Points $(\widehat{Y})$:

```{r y_hat values, echo=FALSE}
Y.a_hat <- (X.a)%*%(Beta.a)
print("Predicted Quarterback Rating Points:")
print(Y.a_hat)
```

Since we have obtained the predicted values, we can compute the residuals and the least squares error as follows:

```{r residuals, echo=FALSE}
Residuals.a <- Y.a - Y.a_hat
print("Deviation between predicted and observed Quarterback Rating Points:")
print(Residuals.a)
```

As the sqare root of sum of squared errors or the residuals vector constitutes the least squares error, we will use `norm` function to compute the length of residuals vector 

```{r least squares, echo=FALSE}
SSR.a <- norm(Residuals.a, "2")
print("The least squares error:")
print(SSR.a)
```
We can also plot the observed Quarterback ratings against the predicted values along with a best fit line using `plot` and `line` functions

```{r plots1, echo=FALSE}
# Observed
#
plot(qbratingdata$`Rating Pts` ~ qbratingdata$`Pct Comp`,main="NFL Quarterback Ratings with respect to Percent Completion", xlab ="Percent Completion", ylab ="Quarterback Rating", col="Blue", pch=16)
# 
# Predicted
#
lines(Y.a_hat ~ qbratingdata$`Pct Comp`)
#
```
```{r residuals1, echo=FALSE}
plot(Residuals.a~qbratingdata$`Pct Comp`, main="Regression Residuals", xlab ="Percent Completion", ylab ="Residuals", col="Red", pch=16)
abline(h=c(0,0))
```
## Objective 2

Let us create multiple linear regression model to relate the response variable __quarterback rating__ to the __percentage of completions__ and __Interceptions__.

```{r x.b_matrix, echo=FALSE}
X.b <- cbind(rep(1,32), qbratingdata$`Pct Comp`, qbratingdata$`Pct Int`)
print("Matrix for Percentage of Completions and Interceptions:")
print(X.b)
```

```{r y.b_vector, echo=FALSE}
Y.b <- cbind(qbratingdata$`Rating Pts`)
print("Vector for Quarterback Rating Points")
print(Y.b)
```

Using `ginv` function, we are going to compute the vector for $\beta$ that consists regression coefficients of $\beta_0$ and $\beta_1$.

```{r Beta.b vector, echo=FALSE}
Beta.b <- ginv(X.b)%*%Y.b
print("Regression Coefficients for Quarterback Rating Points based on Percentage of Completions and ~ Interceptions:")
print(Beta.b)
```

After regression coefficients are obtained, let us compute the predicted values of Quarterback Rating Points  $(\widehat{Y})$, in relation to Percentage of Completions and Interceptions:

```{r y_hat.b, echo=FALSE}
Y.b_hat <- (X.b)%*%(Beta.b)
print("Predicted Quarterback Rating Points:")
print(Y.b_hat)
```
Since we have obtained the predicted values, we can compute the residuals and the least squares error as follows:

```{r residuals_b, echo=FALSE}
Residuals.b <- Y.b - Y.b_hat
print("Deviation between predicted and observed Quarterback Rating Points:")
print(Residuals.b)
```
As the sqareroot of sum of squared errors or the residuals vector constitutes the least squares error, we will use `norm` function to compute the length of residuals vector 
```{r , echo=FALSE}
SSR.b <- norm(Residuals.b, "2")
print("The least squares error:")
print(SSR.b)
```

Based on qbove calculations, you could see following plots about observations points and our regression plane. You can see that our linear assumtion was logical based on information. 


```{r 3D Plot, echo=FALSE}
library(scatterplot3d)
chart<-scatterplot3d(x=qbratingdata$`Pct Comp`,y=qbratingdata$`Int`,z=qbratingdata$`Rating Pts`,color="blue",pch=16,xlab='Percent Completion',ylab='Interceptions',zlab='Quarterback Rating')

chart$plane3d(Beta.b[1],x.coef=Beta.b[2],y.coef=Beta.b[3],lty="dashed")
```


## Objective 3

As we determined the least square errors (square root of sum of squared errors) for each regression model in previous subsections (under Objective 1 and Objective 2), we now can discuss whether using an additional variable(s) improve the accuracy of the regression model for the data set in question?

Let us first revisit the least squares errors for each regression model developed earlier.

```{r objective_3_1, echo=FALSE}
print(paste("The least squares error for simple linear regression is", SSR.a))
print(paste("The least squares error for multiple linear regression model is", SSR.b))
```

As it is the objective of linear regression to minimize the square root of residuals, ignoring other regression diagnostics for the time being, proceeding from the assumption of the smaller the least squares error, better the fit, we can be observed by comparing the least squares errors of linear regression models, the second model which is built on two predictor variables, tends to have less deviation from the actual observed values. Once again, in order to assertain the fitness of a model, we should utilize more sophisticated mathematical tools such as hypothesis testing and confidence intervals. 

<!--- I will edit this paragraph -->

## Objective 4

Using the linear regression models developed in objectives (1) and (2) in parts repsectively, let us try to predict the quarterback rating points for percentage of completions of 60% and percentage of interceptions of 3%.

However, let us start with writing our predictive models below, before plugging in the numbers:

```{r betas_, echo=FALSE}
print("Beta values for simple linear model of objective 1")
print(Beta.a)
#
print("Beta values for multiple linear model of objective 2")
print(Beta.b)
```

$$\widehat{Y_a}=-22.033 + 1.746X_1$$
and

$$\widehat{Y_b}=-9.109 + 1.662X_1 -3.076X_2$$
For the simple linear model, using only the percent of completions (60%), we predict that the quarterback rating point will be:

```{r prediction_a, echo=FALSE}
qbrating.a <- Beta.a[1]+Beta.a[2]*60
cat("For 60% completion rate, the quarterback rating will be", qbrating.a)
```
For the multiple linear model, using the percent of completions (60%) and percent of intercept (3%), we predict that the quarterback rating point will be:

```{r prediction_b, echo=FALSE}
qbrating.b <- Beta.b[1]+Beta.b[2]*60+Beta.b[3]*3
cat("For 60% completion rate and 3% interception rate, the quarterback rating will be", qbrating.b)
```
## Objective 5

 ![](/Users/satah/Downloads/seahawks.jpg) 

# Further Study 
Let us try to use R lm() package and compare our results.
<!-- I will edit this paragraoh -->

```{r further setup libs, include=TRUE}
library(MASS)
library(ISLR)
```

__Simple Linear Regression Using lm() Package__

```{r further_study_1, echo=FALSE}
response <- matrix(qbrating_matrix$`Rating Pts`)
predictor<- matrix(qbrating_matrix$`Pct Comp`)
predictor2 <- matrix(qbrating_matrix$`Pct Int`)
#
#print(response)
#print(predictor)
#
model.fit <- lm(response ~ predictor, data = qbrating_matrix)
#
summary(model.fit)
#
coefficients(model.fit)           # model coefficients
fitted(model.fit)                 # predicted values
residuals(model.fit)              # residuals
# anova(model.fit)                # anova table 
# vcov(model.fit)                 # covariance matrix for model parameters 
#
influence(model.fit)              # regression diagnostics
#
# Plots
#
# Plot observed vs predicted
plot(predictor,response, main="NFL Quarterback Ratings with respect to Percent Completion", xlab ="Percent Completion", ylab ="Quarterback Rating", col="Blue", pch=16)
abline(model.fit)
#
# Plot residuals
#
plot(predict(model.fit), residuals((model.fit)),main="Regression Residuals", xlab ="Percent Completion", ylab ="Residuals", col="Blue", pch=16)
abline(h=c(0,0)) 
#
# Diagnostic plots by R package
#
#layout(matrix(c(1,2,3,4),2,2)) # optional 4 graphs/page 
#plot(model.fit)
#
#
#fit1 <- lm(Y.a~X.a)
#fit2 <- lm(Y.b~X.b)
#
#summary(fit1)
#summary(fit2)
#plot(fit2)
#
#
model.fit2 <- lm(response ~ predictor+predictor2, data = qbrating_matrix)
summary(model.fit2)
#

```


<a name="References"></a>

# References

1. Zarei, A. (2017), _Linear Algebra for Data Science - Lecture Notes_, University of the Pacific, School of Engineering and Computer Science, Stockton, CA. 
</br><br/>
2. Chatterjee, S., and Hadi, A. S. (2012), _Regression Analysis by Example_, 5th ed., New Jersey: John Wiley & Sons.
</br><br/>
3. James, G., Witten, D., Hastie, T., Tibshirani, R. (2013), _An Introduction to Statistical Learning with Applications in R_, 6th printing (2015), New York, Springger Science+Business Media.
</br><br/>
4. Davies, T. M. (2016), _The Book of R: A First Course in Programming and Statistics_, 1st ed., San Francisco: No Starch Press.

# Acknowledgements

The authors wish to thank Dr. A. Zarei for providing guidence and help during the process of completion of this research project. 
